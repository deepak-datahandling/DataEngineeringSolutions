{"cells":[{"cell_type":"markdown","source":["### Weather Details(JSON) Stream processing using **Spark Strucutured Streaming**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"92e6eeca-d706-432d-911f-c5fa89fa13ba"},{"cell_type":"code","source":["from pyspark.sql.functions import *\n","from pyspark.sql import *\n","\n","spark = SparkSession.builder.appName(\"ExtractSchema and Streaming Weather Details\").getOrCreate()\n","\n","#Extract schema from the same json files to feed into Stream query\n","src_tmp_df=spark.read.\\\n","option('multiline','true').\\\n","format('json').\\\n","load(\"Files/WeatherDetails_Stream/*.json\")\n","\n","schema_json=src_tmp_df.schema\n","\n","#CheckPointLocations\n","atmos_check='Files/CheckPoint/AtmosphereDet_Check'\n","cntrydet_check='Files/CheckPoint/CountryDet_Check'\n","cntrymstr_check='Files/CheckPoint/CountryMstr_Check'\n","landdet_check='Files/CheckPoint/LandmarkDet_Check'\n","weatherdet_check='Files/CheckPoint/WeatherDet_Check'\n","\n","#Begin the Streaming\n","src_df=spark.readStream.\\\n","schema(schema_json).\\\n","option('multiline','true').\\\n","format('json').\\\n","load(\"Files/WeatherDetails_Stream/*.json\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3c279c0d-2ada-4939-8e1d-b7e6022f08c7"},{"cell_type":"code","source":["#read Country Details Table\n","country_df = spark.sql(\"SELECT * FROM LHDEV.countrydetails\")\n","Joined_df=src_df.join(country_df,col('location.country')==col('Country'),'inner')\\\n","                .filter(col('location.region')!='Anhui')\\\n","                .filter(col('location.region')!='Ghazni')\\\n","                .withColumn('WeatherID',col('current.weather_code').cast('int'))\\\n","                .withColumn('AtmosphereID',concat(col('CountryCode'),substring(col('location.country'),-3,3)))\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"8286bf8e-3c8b-4f87-8b50-ed3485f87977"},{"cell_type":"code","source":["#ID Details -- FactTable\n","#SCD Type - 1\n","fact_df=Joined_df.withColumn('Load_TS',current_timestamp()).select(['CountryCode','WeatherID','AtmosphereID','Load_TS'])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"be1d349c-2a5e-4bc1-8a16-3318fc699373"},{"cell_type":"code","source":["#countryDetails --DimentionTable\n","#SCD Type - 1\n","country_df_final=Joined_df.withColumn('City',split(col('request.query'),',').getItem(0))\\\n","                        .withColumn('Latitude',col('location.lat').cast('decimal(8,6)'))\\\n","                        .withColumn('Longitude',col('location.lon').cast('decimal(9,6)'))\\\n","                        .select(['CountryCode','City','Country','Region','Latitude','Longitude']).distinct().dropDuplicates(['CountryCode'])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"53343668-a7d0-40a0-8a4d-78f1dde1f98a"},{"cell_type":"code","source":["#AtmosphereDetails --DimentionTable\n","#SCD Type - 2\n","atmosphere_df=Joined_df.withColumn('CloundCover',concat(col('current.cloudcover').cast('int'),lit('%')))\\\n","                        .withColumn('Humidity',concat(col('current.humidity').cast('int'),lit('%')))\\\n","                        .withColumn('is_day',col('current.is_day'))\\\n","                        .withColumn('temperature',concat(col('current.temperature').cast('int'),lit('Â°C')))\\\n","                        .withColumn('wind_degree',concat(col('current.wind_degree').cast('int'),lit('%')))\\\n","                        .withColumn('wind_direction',col('current.wind_dir'))\\\n","                        .withColumn('wind_speed',concat(col('current.wind_speed').cast('int'),lit(' Km/hr')))\\\n","                        .withColumn('observation_time',to_timestamp((concat(substring(col('location.localtime'),1,10),col('current.observation_time'))),'yyyy-MM-ddhh:mm a'))\\\n","                        .withColumn('is_latest',lit(1))\\\n","                        .select(['AtmosphereID','observation_time','temperature','Humidity','CloundCover','wind_direction','wind_degree','wind_speed','is_day','is_latest','WeatherID'])\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4c9dd9ac-260d-4f17-af65-cbfe7bb36517"},{"cell_type":"code","source":["#WeatherDetails --DimentionTable\n","#SCD Type - 1\n","weather_df=Joined_df.withColumn('weather_descriptions',col('current.weather_descriptions')[0])\\\n","                    .select(['WeatherID','weather_descriptions']).distinct().dropDuplicates(['WeatherID'])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f5bfd316-af57-4018-b432-225ac077a888"},{"cell_type":"code","source":["#Creating Dict of ID and TableNames for future uses\n","tble_dict={'CountryCode':'CountryDetFinal','WeatherID':'WeatherDet','AtmosphereID':'AtmosphereDet'}"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e12ce822-b4dd-4961-8ce9-f90f51f3e165"},{"cell_type":"code","source":["def process_batch_all(df, batch_id,id_col):\n","    '''\n","    A function which takes inputs as dataframe(df), unique id of stream batch(batch id), list of primary key ID columns (id_col) and perform the MERGE opertion\n","    Based on the certain conditions.\n","    '''\n","    if len(id_col) == 3 :\n","        df.createOrReplaceTempView(\"incoming_df\")\n","        df._jdf.sparkSession().sql(f'''MERGE INTO countrymstr T USING incoming_df A on T.CountryCode=A.CountryCode\n","                    WHEN MATCHED THEN UPDATE SET\n","                    T.WeatherID=A.WeatherID,\n","                    T.AtmosphereID=A.AtmosphereID,\n","                    T.Load_TS=CURRENT_TIMESTAMP\n","                    WHEN NOT MATCHED THEN\n","                    INSERT\n","                    * ''')\n","    elif len(id_col) == 2:\n","        df.createOrReplaceTempView(\"incoming_df\")\n","        df._jdf.sparkSession().sql(f'''MERGE INTO atmospheredet T USING incoming_df A on T.is_latest=A.is_latest\n","                    AND T.AtmosphereID=A.AtmosphereID\n","                    AND T.observation_time <= A.observation_time\n","                    WHEN MATCHED THEN UPDATE SET T.is_latest=0\n","                    ''')\n","        df._jdf.sparkSession().sql(f'''MERGE INTO atmospheredet T USING incoming_df A on T.is_latest=A.is_latest\n","                    AND T.AtmosphereID=A.AtmosphereID\n","                    AND T.observation_time <= A.observation_time\n","                    WHEN NOT MATCHED THEN\n","                    INSERT\n","                    * ''')\n","    else:    \n","        tble=tble_dict[id_col[0]]\n","        df.createOrReplaceTempView(\"incoming_df\")\n","        df._jdf.sparkSession().sql(f'''MERGE INTO {tble} T USING incoming_df A on T.{id_col[0]}=A.{id_col[0]}\n","                    WHEN NOT MATCHED THEN\n","                    INSERT\n","                    * ''')\n","\n","def process_batch(df, batch_id):\n","    '''\n","    A function which is called by foreachBatch to process the streaming data into small batches which is act like a normal dataframe.\n","    '''\n","    id_col=[x for x in df.columns if x in ['CountryCode','WeatherID','AtmosphereID']]\n","    process_batch_all(df, batch_id, id_col)\n","\n","def streamwriter(final_df,chckdir):\n","    '''\n","    A function which begin the Stream Writing and return the results.\n","    '''\n","    query=final_df.writeStream.foreachBatch(process_batch)\\\n","    .option(\"checkpointLocation\", chckdir)\\\n","    .trigger(processingTime='2 seconds')\\\n","    .outputMode(\"append\")\\\n","    .start()\n","    return query"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"973b93c1-6920-4358-9171-7f1b1988a92e"},{"cell_type":"code","source":["#Loading CountryMaster Table\n","countrymstr=streamwriter(fact_df,cntrymstr_check)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"291fe465-2bb1-4931-9010-275b0e66c892"},{"cell_type":"code","source":["\n","#Loading Atmosphere Details Table\n","atmospheredet=streamwriter(atmosphere_df,atmos_check)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"44a4b919-01e8-4451-a5e7-190fe1acc759"},{"cell_type":"code","source":["#Loading Country Details Final Table\n","countrydetfinal=streamwriter(country_df_final,cntrydet_check)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8fe6b2a3-e07c-42fe-8449-e5d6124e8e1f"},{"cell_type":"code","source":["#Loading Weather Details Table\n","weatherdet=streamwriter(weather_df,weatherdet_check)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e26bdb53-f9cc-4e2c-9283-349cd017d9dc"},{"cell_type":"code","source":["#Query to check the streaming is running or not. Also, helps to stop the streaming\n","for stream in spark.streams.active:\n","    s = spark.streams.get(stream.id)\n","    print(s)\n","    #s.stop()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f0fb90dd-8fd7-4df0-94b5-395a50189ed5"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"780e9978-6322-4fcd-a5cc-be63a6622edc","default_lakehouse_name":"LHDEV","default_lakehouse_workspace_id":"7b0d371b-149a-4df5-b9ef-5f580e3f2f0a"}}},"nbformat":4,"nbformat_minor":5}