{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9abc2c07-28aa-4d17-b29d-d4a9d8cbaf19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import com.databricks.spark.xml.functions.from_xml\n",
    "import com.databricks.spark.xml.schema_of_xml\n",
    "import spark.implicits._\n",
    "import com.databricks.spark.xml._\n",
    "\n",
    "val toStrUDF = udf((bytes: Array[Byte]) => new String(bytes, \"UTF-8\")) \n",
    "// UDF to convert the binary to String\n",
    " \n",
    "val df_schema = spark.read.format(\"binaryFile\").load(\"/FileStore/DSR/Source/Stream\").select(toStrUDF($\"content\").alias(\"text\"))\n",
    "  \n",
    "val payloadSchema = schema_of_xml(df_schema.select(\"text\").as[String]) \n",
    "// This is costlier operation when we have too many files because of file-listing schema inference, it is best to use the user-defined custom schema \n",
    "\n",
    "val df = spark.readStream.format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.useNotifications\", \"false\")\n",
    "  .option(\"cloudFiles.format\", \"binaryFile\")\n",
    "  .option(\"cloudFiles.maxFilesPerTrigger\", 2)\n",
    "  .load(\"/FileStore/DSR/Source/Stream\")\n",
    "  .select(toStrUDF($\"content\").alias(\"text\")).select(from_xml($\"text\", payloadSchema).alias(\"structuredBody\"))\n",
    "\n",
    "display(df)\n",
    "df.createOrReplaceTempView(\"dfp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50bbd4c7-3e69-4b87-8ef8-b9c21c25a4b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Variables\n",
    "Checkpoint_directory_addr = \"/FileStore/DSR/CheckPointing/Proc_Addr_Details\"\n",
    "Checkpoint_directory_proc = \"/FileStore/DSR/CheckPointing/Proc_Details\"\n",
    "Proc_Sample_File = \"/FileStore/DSR/Source/Sample/Procedure_List_Samples.csv\"\n",
    "Addr_Sample_File = \"/FileStore/DSR/Source/Sample/Address_List_Samples.csv\"\n",
    "Proc_Target = \"dsr.Proc_Details_managed\"\n",
    "Addr_Target = \"dsr.Proc_Addr_Details_managed\"\n",
    "src_df = sqlContext.table(\"dfp\")\n",
    "\n",
    "# XML File Handling\n",
    "temp_df1 = src_df.select(\n",
    "    explode(src_df.structuredBody.component.structuredBody.component.section).alias(\n",
    "        \"Source\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Filter out Procedures Details alone\n",
    "temp_df2 = temp_df1.filter(temp_df1.Source.code._displayName == \"HISTORY OF PROCEDURES\")\n",
    "\n",
    "# Below \"proc_det\" Dataframe used later in order to extract procedure details which is in String of XML format\n",
    "proc_det = temp_df2.select(temp_df2.Source.text.alias(\"proc_det\"))\n",
    "\n",
    "# XML Handling Continues\n",
    "temp_df2 = temp_df2.select(explode(temp_df1.Source.entry).alias(\"src\"))\n",
    "temp_df3 = temp_df2.filter(temp_df2.src.procedure.isNotNull()).select(\n",
    "    temp_df2.src.procedure.performer.assignedEntity.Addr.alias(\"addr\")\n",
    ")\n",
    "temp_df4 = temp_df3.select(explode(temp_df3.addr).alias(\"addr\"))\n",
    "\n",
    "temp_df5 = (\n",
    "    temp_df4.withColumn(\"id\", split(temp_df4.addr.id, \",\"))\n",
    "    .withColumn(\"Hospital\", temp_df4.addr.Hospital)\n",
    "    .withColumn(\"Street\", temp_df4.addr.streetAddressLine)\n",
    "    .withColumn(\"City\", temp_df4.addr.city)\n",
    "    .withColumn(\"State\", temp_df4.addr.state)\n",
    "    .withColumn(\"Country\", temp_df4.addr.country)\n",
    "    .withColumn(\"PostalCode\", temp_df4.addr.postalCode)\n",
    ")\n",
    "\n",
    "addr_columns = [\"Hospital\", \"Street\", \"City\", \"State\", \"Country\", \"PostalCode\"]\n",
    "temp_df5 = temp_df5.select(explode(temp_df5.id).alias(\"PID\"), *addr_columns)\n",
    "\n",
    "final_add_df = temp_df5.select(col(\"PID\").cast(LongType()).alias(\"PID\"), *addr_columns)\n",
    "\n",
    "# Processing Procedure Details Which is in String of XML\n",
    "def process(df):\n",
    "    lk = []\n",
    "    for k in range(0, df.count()):\n",
    "        xml = df.collect()[k][\"proc_det\"]\n",
    "        doc = ET.fromstring(xml)\n",
    "        for i in range(0, len(doc[1])):\n",
    "            lk.append(doc[1][i].attrib[\"ID\"])\n",
    "            for j in range(0, 3):\n",
    "                if j == 1:\n",
    "                    lk.append(doc[1][i][j].text[0:11])\n",
    "                else:\n",
    "                    lk.append(doc[1][i][j].text)\n",
    "    lk_o = [tuple(lk[i : i + 4]) for i in range(0, len(lk), 4)]\n",
    "    return lk_o\n",
    "\n",
    "\n",
    "def process_micro_batch(df, batch_id):\n",
    "    columns = [\"PID\", \"Name\", \"Date\", \"Status\"]\n",
    "    proc_temp = spark.createDataFrame(process(df), columns)\n",
    "\n",
    "    new_streaming_df = proc_temp.select(\n",
    "        proc_temp.PID.cast(LongType()),\n",
    "        \"Name\",\n",
    "        to_date(col(\"Date\"), \"dd MMM yyyy\").alias(\"Proc_Date\"),\n",
    "        \"Status\",\n",
    "    )\n",
    "    new_streaming_df.createOrReplaceTempView(\"proc\")\n",
    "    spark.sql(\n",
    "        f\"\"\"MERGE INTO {Proc_Target} T USING proc P on T.PID = P.PID\n",
    "              and T.Name = P.Name\n",
    "              and T.Proc_Date = P.Proc_Date\n",
    "              WHEN NOT MATCHED THEN\n",
    "              INSERT\n",
    "              *\"\"\"\n",
    "    )\n",
    "\n",
    "def addr_process_micro_batch(df, batch_id):\n",
    "    df.createOrReplaceTempView(\"addr\")\n",
    "    df._jdf.sparkSession().sql(f'''MERGE INTO {Addr_Target} T USING addr A on T.PID=A.PID\n",
    "                    and T.Hospital=A.Hospital\n",
    "                    WHEN NOT MATCHED THEN\n",
    "                    INSERT\n",
    "                    * ''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "996451d7-e8f2-4aaa-9077-d50005f09ab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_proc = (\n",
    "    proc_det.writeStream.foreachBatch(process_micro_batch)\n",
    "    .option(\"checkpointLocation\", Checkpoint_directory_proc)\n",
    "    .trigger(processingTime=\"1 minute\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4808129-8447-4fd0-b518-ec0407ebf5e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Writing date into Delta Table\n",
    "query_addr = (\n",
    "    final_add_df.writeStream.foreachBatch(addr_process_micro_batch)\n",
    "    .option(\"checkpointLocation\", Checkpoint_directory_addr)\n",
    "    .trigger(processingTime=\"1 minute\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "264ce107-1842-41da-b77c-a5b213399c52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select\n",
    "  *\n",
    "from\n",
    "  dsr.Proc_Details_managed\n",
    "order by\n",
    "  PID;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac35f281-4c48-4c52-90bc-1508a8327a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select\n",
    "  *\n",
    "from\n",
    "  dsr.Proc_Addr_Details_managed\n",
    "order by\n",
    "  PID;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda41461-5a92-42ef-aae0-e700f1f69ce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop Streaming\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    s = spark.streams.get(stream.id)\n",
    "    print(s)\n",
    "    s.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4059b138-837e-4855-9f4e-fdba627431d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410 Sample Data Loaded Successfully in dsr.Proc_Details_managed Table\n1400 Sample Data Loaded Successfully in dsr.Proc_Addr_Details_managed Table\n"
     ]
    }
   ],
   "source": [
    "# Sample Data Loading BLOCK\n",
    "\n",
    "\n",
    "def load_proc_sampl(loc, load_table):\n",
    "    length = 0\n",
    "    if bool(dbutils.fs.ls(loc)) is True:\n",
    "        sample = spark.read.format(\"csv\").option(\"header\", True).csv(loc)\n",
    "        length = sample.count()\n",
    "        sample.createOrReplaceTempView(\"sample\")\n",
    "        spark.sql(f\"\"\"insert into {load_table} select * from sample\"\"\")\n",
    "    else:\n",
    "        raise Exception()\n",
    "    return length\n",
    "\n",
    "\n",
    "# Load Proc_Sample Data\n",
    "try:\n",
    "    print(\n",
    "        f\"{load_proc_sampl(Proc_Sample_File, Proc_Target)} Sample Data Loaded Successfully in {Proc_Target} Table\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    if \"java.io.FileNotFoundException\" in str(e):\n",
    "        print(f\"Sample Data File Not Found {Proc_Target} Table\")\n",
    "    else:\n",
    "        print(str(e))\n",
    "\n",
    "# Load Addr_Sample Data\n",
    "try:\n",
    "    print(\n",
    "        f\"{load_proc_sampl(Addr_Sample_File, Addr_Target)} Sample Data Loaded Successfully in {Addr_Target} Table\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    if \"java.io.FileNotFoundException\" in str(e):\n",
    "        print(f\"Sample Data File Not Found {Addr_Target} Table\")\n",
    "    else:\n",
    "        print(str(e))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1246115617133945,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DSR - XML Stream Soltuion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}